{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514c1c97-8ddb-4584-93e7-3e00f5647a2e",
   "metadata": {},
   "source": [
    "## üìò Notebook 1: Data Preparation & Analysis\n",
    "\n",
    "**Funcionalidades:**\n",
    "1. Carrega dataset da estrutura de pastas (data/odir5k/)\n",
    "2. Data Loader otimizado para a nova estrutura\n",
    "3. Data Augmentation para treino\n",
    "4. Visualiza√ß√µes e estat√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5c933-b703-423c-84b8-118b69644199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b4da3-de98-4a5f-a200-ec92f0b712c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pega no caminho da pasta onde o notebook est√° (ex: .../notebooks)\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "# Define o PROJECT_ROOT como sendo a pasta \"m√£e\" (odir-project)\n",
    "PROJECT_ROOT = os.path.dirname(CURRENT_DIR) \n",
    "\n",
    "# Agora o DATASET_ROOT apontar√° corretamente para odir-project/data/odir5k\n",
    "DATASET_ROOT = os.path.join(PROJECT_ROOT, \"data\", \"odir5k\")\n",
    "\n",
    "print(f\"Caminho do Notebook: {CURRENT_DIR}\")\n",
    "print(f\"Raiz do Projeto: {PROJECT_ROOT}\")\n",
    "print(f\"Caminho do Dataset: {DATASET_ROOT}\")\n",
    "\n",
    "# Verificar se agora o caminho est√° correto\n",
    "if os.path.exists(DATASET_ROOT):\n",
    "    print(\"‚úÖ Sucesso: Pasta de dados encontrada!\")\n",
    "else:\n",
    "    print(\"‚ùå Erro: A pasta ainda n√£o foi encontrada. Verifica se o nome √© 'odir5k' ou 'ODIR-5K'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8508d1-0249-4534-9cc8-9a7450df377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDataset(Dataset):\n",
    "    \"\"\"Dataset ODIR-5K usando estrutura de pastas\"\"\"\n",
    "    \n",
    "    def __init__(self, split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.img_dir = os.path.join(DATASET_ROOT, split)\n",
    "        \n",
    "        # Carregar metadados\n",
    "        metadata_path = os.path.join(self.img_dir, f\"{split}_metadata.csv\")\n",
    "        self.data = pd.read_csv(metadata_path)\n",
    "        self.disease_cols = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "        self.image_files = [f for f in os.listdir(self.img_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        print(f\"  üìÇ {split}: {len(self.data)} pacients, {len(self.image_files)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        patient_id = str(row['ID'])\n",
    "        \n",
    "        img_path = None\n",
    "        for side in ['left', 'right']:\n",
    "            img_name = f\"{patient_id}_{side}.jpg\"\n",
    "            full_path = os.path.join(self.img_dir, img_name)\n",
    "            if os.path.exists(full_path):\n",
    "                img_path = full_path\n",
    "                break\n",
    "        \n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Imagem n√£o encontrada para ID {patient_id}\")\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        labels = torch.tensor([row[col] for col in self.disease_cols], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, labels, patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23064112-d97f-4af1-aa85-b82ae58e3b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropOnly(object):\n",
    "        \"\"\"S√≥ cropping, SEM CLAHE\"\"\"\n",
    "        \n",
    "        def __call__(self, img):\n",
    "            img = np.array(img)\n",
    "            \n",
    "            # Mesmo cropping que ApplyCLAHEandCrop_Adaptive mas SEM CLAHE\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            mask = gray > 10\n",
    "            if np.any(mask):\n",
    "                coords = np.argwhere(mask)\n",
    "                y0, x0 = coords.min(axis=0)\n",
    "                y1, x1 = coords.max(axis=0) + 1\n",
    "                img = img[y0:y1, x0:x1]\n",
    "            \n",
    "            return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9e622-a767-4682-ae19-8187860b4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyCLAHEandCrop_Adaptive(object):\n",
    "    \"\"\"\n",
    "    CLAHE adaptativo: s√≥ aplica em imagens de baixo contraste\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.contrast_threshold = 50   # ‚Üê OTIMIZADO!\n",
    "        self.clip_limit = 3.0          # Suave\n",
    "        self.tile_grid_size = (8, 8)\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # To numpy (opencv)\n",
    "        img = np.array(img)\n",
    "        \n",
    "        # ==================== CROPPING ====================\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray > 10\n",
    "        \n",
    "        if np.any(mask):\n",
    "            coords = np.argwhere(mask)\n",
    "            y0, x0 = coords.min(axis=0)\n",
    "            y1, x1 = coords.max(axis=0) + 1\n",
    "            img = img[y0:y1, x0:x1]\n",
    "        \n",
    "        # ==================== MEDIR CONTRASTE ====================\n",
    "        gray_cropped = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        contrast = gray_cropped.std()  # Standard deviation = contraste\n",
    "        \n",
    "        # ==================== CLAHE ADAPTATIVO ====================\n",
    "        # S√ì aplicar se contraste baixo!\n",
    "        if contrast < self.contrast_threshold:\n",
    "            # Converter para LAB\n",
    "            lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            \n",
    "            # Aplicar CLAHE no canal L\n",
    "            clahe = cv2.createCLAHE(\n",
    "                clipLimit=self.clip_limit,\n",
    "                tileGridSize=self.tile_grid_size\n",
    "            )\n",
    "            l = clahe.apply(l)\n",
    "            \n",
    "            # Merge e converter de volta\n",
    "            img = cv2.merge([l, a, b])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n",
    "        \n",
    "        # To PIL\n",
    "        return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54079a7-2b95-4a15-ba98-7591ba07dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_train_transform_CLAHE():\n",
    "    return transforms.Compose([\n",
    "        ApplyCLAHEandCrop_Adaptive(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_test_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_test_transform_CLAHE():\n",
    "    return transforms.Compose([\n",
    "        ApplyCLAHEandCrop_Adaptive(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3712ad-d281-47a3-9d99-fddac27062df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_step_by_step(dataset_basic, dataset_clahe, num_samples=3):\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(12, 10))\n",
    "    fig.suptitle('Image Processing Evolution (Original -> Augmentation -> CLAHE + Crop', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # 1. ORIGINAL IMAGE\n",
    "        row = dataset_basic.data.iloc[i]\n",
    "        patient_id = str(row['ID'])\n",
    "        img_name = f\"{patient_id}_left.jpg\"\n",
    "        img_path = os.path.join(dataset_basic.img_dir, img_name)\n",
    "        if not os.path.exists(img_path):\n",
    "            img_path = os.path.join(dataset_basic.img_dir, f\"{patient_id}_right.jpg\")\n",
    "        \n",
    "        raw_img = Image.open(img_path).convert('RGB')\n",
    "        raw_img = raw_img.resize((224, 224))\n",
    "        axes[0, i].imshow(raw_img)\n",
    "        axes[0, i].set_title(f'1. Original (ID: {patient_id})')\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        # 2. AUGMENTATION ONLY\n",
    "        img_aug, _, _ = dataset_basic[i]\n",
    "        img_aug = img_aug.permute(1, 2, 0).numpy()\n",
    "        img_aug = img_aug * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        axes[1, i].imshow(np.clip(img_aug, 0, 1))\n",
    "        axes[1, i].set_title('2. Only Augmentation')\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "        # 3. CLAHE + CROP\n",
    "        img_final, _, _ = dataset_clahe[i]\n",
    "        img_final = img_final.permute(1, 2, 0).numpy()\n",
    "        img_final = img_final * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        axes[2, i].imshow(np.clip(img_final, 0, 1))\n",
    "        axes[2, i].set_title('3. FINAL (B): CLAHE + Augment')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/evolution_examples.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def plot_dataset_statistics(save_path='results/dataset_statistics.png'):\n",
    "    train_data = pd.read_csv(os.path.join(DATASET_ROOT, 'train', 'train_metadata.csv'))\n",
    "    disease_names = {'N': 'Normal', 'D': 'Diabetes', 'G': 'Glaucoma', 'C': 'Cataract', \n",
    "                     'A': 'AMD', 'H': 'Hypertension', 'M': 'Myopia', 'O': 'Other'}\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Dataset Statistics - Training Set', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Distribui√ß√£o de doen√ßas\n",
    "    disease_counts = train_data[list(disease_names.keys())].sum().sort_values(ascending=False)\n",
    "    axes[0, 0].bar(range(len(disease_counts)), disease_counts.values, color='steelblue')\n",
    "    axes[0, 0].set_xticks(range(len(disease_counts)))\n",
    "    axes[0, 0].set_xticklabels([disease_names[k] for k in disease_counts.index], rotation=45, ha='right')\n",
    "    axes[0, 0].set_title('Disease Distribution')\n",
    "    \n",
    "    # 2. Distribui√ß√£o de idade\n",
    "    axes[0, 1].hist(train_data['Patient Age'].dropna(), bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_title('Age Distribution')\n",
    "    \n",
    "    # 3. Distribui√ß√£o por sexo\n",
    "    sex_counts = train_data['Patient Sex'].value_counts()\n",
    "    axes[1, 0].pie(sex_counts.values, labels=sex_counts.index, autopct='%1.1f%%', colors=['lightblue', 'lightpink'], startangle=90)\n",
    "    axes[1, 0].set_title('Sex Distribution')\n",
    "    \n",
    "    # 4. Doen√ßas por paciente\n",
    "    diseases_per_patient = train_data[list(disease_names.keys())].sum(axis=1)\n",
    "    axes[1, 1].hist(diseases_per_patient, bins=range(0, 9), color='mediumseagreen', edgecolor='black', alpha=0.7, align='left')\n",
    "    axes[1, 1].set_title('Diseases per Pacient')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show() # Garante que aparece no Notebook\n",
    "    print(f\"  Estat√≠sticas guardadas em: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe793a-c64f-4985-9ad7-8b7eef2d3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verifica√ß√£o de Seguran√ßa e Execu√ß√£o ---\n",
    "if not os.path.exists(DATASET_ROOT):\n",
    "    print(f\"‚ùå ERRO: Dataset n√£o encontrado em {DATASET_ROOT}\")\n",
    "else:\n",
    "    # 1. Criar Datasets Reais\n",
    "    train_dataset_basic = ODIRDataset('train', transform=get_train_transform())\n",
    "    train_dataset_clahe = ODIRDataset('train', transform=get_train_transform_CLAHE())\n",
    "    \n",
    "    val_dataset = ODIRDataset('val', transform=get_val_test_transform_CLAHE())\n",
    "    test_dataset = ODIRDataset('test', transform=get_val_test_transform_CLAHE())\n",
    "\n",
    "    visualize_step_by_step(train_dataset_basic, train_dataset_clahe)\n",
    "    \n",
    "    plot_dataset_statistics()\n",
    "\n",
    "    print(\"\\n Resume: Train:\", len(train_dataset_clahe), \"| Val:\", len(val_dataset), \"| Test:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a618fc6-38e5-4cac-ae3a-67180252ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_summary():\n",
    "    \"\"\"Tabela formal para relat√≥rio\"\"\"\n",
    "    train_data = pd.read_csv(os.path.join(DATASET_ROOT, 'train', 'train_metadata.csv'))\n",
    "    val_data = pd.read_csv(os.path.join(DATASET_ROOT, 'val', 'val_metadata.csv'))\n",
    "    test_data = pd.read_csv(os.path.join(DATASET_ROOT, 'test', 'test_metadata.csv'))\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'Split': ['Training', 'Validation', 'Test', 'Total'],\n",
    "        'Patients': [\n",
    "            len(train_data['ID'].unique()),\n",
    "            len(val_data['ID'].unique()),\n",
    "            len(test_data['ID'].unique()),\n",
    "            len(train_data['ID'].unique()) + len(val_data['ID'].unique()) + len(test_data['ID'].unique())\n",
    "        ],\n",
    "        'Images': [len(train_data), len(val_data), len(test_data), \n",
    "                   len(train_data) + len(val_data) + len(test_data)],\n",
    "        'Percentage': [\n",
    "            f\"{len(train_data)/(len(train_data)+len(val_data)+len(test_data))*100:.1f}%\",\n",
    "            f\"{len(val_data)/(len(train_data)+len(val_data)+len(test_data))*100:.1f}%\",\n",
    "            f\"{len(test_data)/(len(train_data)+len(val_data)+len(test_data))*100:.1f}%\",\n",
    "            \"100%\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET SUMMARY - ODIR-5K\")\n",
    "    print(\"=\"*60)\n",
    "    print(summary.to_string(index=False))\n",
    "    \n",
    "    # Salvar para LaTeX\n",
    "    with open('results/dataset_summary.tex', 'w') as f:\n",
    "        f.write(summary.to_latex(index=False))\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83122873-d105-419f-9a2d-dbdddc7b960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_disease_cooccurrence(train_data, save_path='results/cooccurrence_heatmap.png'):\n",
    "    \"\"\"Heatmap de correla√ß√£o entre doen√ßas\"\"\"\n",
    "    disease_names = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "    disease_full = {\n",
    "        'N': 'Normal', 'D': 'Diabetes', 'G': 'Glaucoma', \n",
    "        'C': 'Cataract', 'A': 'AMD', 'H': 'Hypertension',\n",
    "        'M': 'Myopia', 'O': 'Other'\n",
    "    }\n",
    "    \n",
    "    # Criar matriz de co-occurrence\n",
    "    disease_matrix = train_data[disease_names].values\n",
    "    cooccurrence = np.dot(disease_matrix.T, disease_matrix)\n",
    "    \n",
    "    # Normalizar por diagonal (Jaccard similarity)\n",
    "    diag = np.diag(cooccurrence)\n",
    "    normalized = cooccurrence / (diag[:, None] + diag[None, :] - cooccurrence + 1e-8)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(normalized, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "                xticklabels=[disease_full[d] for d in disease_names],\n",
    "                yticklabels=[disease_full[d] for d in disease_names],\n",
    "                cbar_kws={'label': 'Jaccard Similarity'})\n",
    "    \n",
    "    plt.title('Disease Co-occurrence Matrix (Training Set)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Encontrar pares mais comuns\n",
    "    print(\"\\nüîç Most Common Disease Pairs:\")\n",
    "    pairs = []\n",
    "    for i in range(len(disease_names)):\n",
    "        for j in range(i+1, len(disease_names)):\n",
    "            count = cooccurrence[i, j]\n",
    "            if count > 0:\n",
    "                pairs.append((disease_full[disease_names[i]], \n",
    "                             disease_full[disease_names[j]], \n",
    "                             int(count)))\n",
    "    \n",
    "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    for d1, d2, count in pairs[:10]:\n",
    "        print(f\"  {d1} + {d2}: {count} cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf613959-b789-4e4b-805c-8a6d25fdf978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_multilabel_distribution(train_data):\n",
    "    \"\"\"An√°lise de quantas doen√ßas por paciente\"\"\"\n",
    "    disease_names = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "    \n",
    "    # Contar doen√ßas por imagem\n",
    "    diseases_per_image = train_data[disease_names].sum(axis=1)\n",
    "    \n",
    "    print(\"\\n Multi-label Statistics:\")\n",
    "    print(f\"  Images with 0 diseases: {(diseases_per_image == 0).sum()} ({(diseases_per_image == 0).sum()/len(train_data)*100:.1f}%)\")\n",
    "    print(f\"  Images with 1 disease: {(diseases_per_image == 1).sum()} ({(diseases_per_image == 1).sum()/len(train_data)*100:.1f}%)\")\n",
    "    print(f\"  Images with 2+ diseases: {(diseases_per_image >= 2).sum()} ({(diseases_per_image >= 2).sum()/len(train_data)*100:.1f}%)\")\n",
    "    print(f\"  Max diseases per image: {diseases_per_image.max()}\")\n",
    "    print(f\"  Mean diseases per image: {diseases_per_image.mean():.2f}\")\n",
    "    \n",
    "    # Estat√≠sticas por doen√ßa\n",
    "    print(\"\\nüìà Per-Disease Statistics:\")\n",
    "    for disease in disease_names:\n",
    "        count = train_data[disease].sum()\n",
    "        pct = count / len(train_data) * 100\n",
    "        full_name = {'N': 'Normal', 'D': 'Diabetes', 'G': 'Glaucoma', \n",
    "                     'C': 'Cataract', 'A': 'AMD', 'H': 'Hypertension',\n",
    "                     'M': 'Myopia', 'O': 'Other'}[disease]\n",
    "        print(f\"  {full_name:15s}: {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2decd-ecf4-4f32-9c88-56c851002f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_quality(dataset, num_samples=500, save_path='results/quality_analysis.png'):\n",
    "    \"\"\"An√°lise de qualidade das imagens\"\"\"\n",
    "    print(f\"\\n Analyzing image quality ({num_samples} samples)...\")\n",
    "    \n",
    "    intensities = []\n",
    "    contrasts = []\n",
    "    black_border_percentages = []\n",
    "    \n",
    "    for i in tqdm(range(min(num_samples, len(dataset)))):\n",
    "        img, _, _ = dataset[i]\n",
    "        \n",
    "        # Converter para numpy (se for tensor)\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Grayscale para an√°lise\n",
    "        gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Intensidade m√©dia\n",
    "        intensities.append(gray.mean())\n",
    "        \n",
    "        # Contraste (std da intensidade)\n",
    "        contrasts.append(gray.std())\n",
    "        \n",
    "        # % de bordas pretas\n",
    "        black_pixels = (gray < 10).sum()\n",
    "        black_border_percentages.append(black_pixels / gray.size * 100)\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 1. Distribui√ß√£o de intensidade\n",
    "    axes[0].hist(intensities, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(np.mean(intensities), color='red', linestyle='--', \n",
    "                    label=f'Mean: {np.mean(intensities):.1f}')\n",
    "    axes[0].set_xlabel('Mean Pixel Intensity')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Intensity Distribution')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # 2. Distribui√ß√£o de contraste\n",
    "    axes[1].hist(contrasts, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(np.mean(contrasts), color='red', linestyle='--',\n",
    "                    label=f'Mean: {np.mean(contrasts):.1f}')\n",
    "    axes[1].set_xlabel('Contrast (Std Dev)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Contrast Distribution')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # 3. % de bordas pretas\n",
    "    axes[2].hist(black_border_percentages, bins=30, color='mediumseagreen', \n",
    "                 edgecolor='black', alpha=0.7)\n",
    "    axes[2].axvline(np.mean(black_border_percentages), color='red', linestyle='--',\n",
    "                    label=f'Mean: {np.mean(black_border_percentages):.1f}%')\n",
    "    axes[2].set_xlabel('Black Border (%)')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    axes[2].set_title('Black Border Distribution')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.suptitle(f'Image Quality Analysis (n={num_samples})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n Quality Metrics:\")\n",
    "    print(f\"  Mean intensity: {np.mean(intensities):.1f} ¬± {np.std(intensities):.1f}\")\n",
    "    print(f\"  Mean contrast: {np.mean(contrasts):.1f} ¬± {np.std(contrasts):.1f}\")\n",
    "    print(f\"  Mean black border: {np.mean(black_border_percentages):.1f}% ¬± {np.std(black_border_percentages):.1f}%\")\n",
    "    print(f\"  Images with >10% black border: {(np.array(black_border_percentages) > 10).sum()} ({(np.array(black_border_percentages) > 10).sum()/num_samples*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71921cf-2fc4-457b-bce5-65460f654175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_complete_pipeline(num_samples=3, save_path='results/pipeline_simple.png'):\n",
    "    \"\"\"Vers√£o simplificada: Original -> Baseline -> CLAHE\"\"\"\n",
    "    \n",
    "    print(f\"\\n Generating pipeline visualization ({num_samples} samples)...\")\n",
    "    \n",
    "    dataset_baseline = ODIRDataset('train', transform=get_val_test_transform())\n",
    "    dataset_clahe = ODIRDataset('train', transform=get_val_test_transform_CLAHE())\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle('Preprocessing Pipeline: Baseline vs CLAHE', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Baseline (sem CLAHE)\n",
    "        img_base, labels, patient_id = dataset_baseline[i]\n",
    "        img_base_np = img_base.permute(1, 2, 0).numpy()\n",
    "        img_base_np = img_base_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img_base_np = np.clip(img_base_np, 0, 1)\n",
    "        \n",
    "        # CLAHE\n",
    "        img_clahe, _, _ = dataset_clahe[i]\n",
    "        img_clahe_np = img_clahe.permute(1, 2, 0).numpy()\n",
    "        img_clahe_np = img_clahe_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img_clahe_np = np.clip(img_clahe_np, 0, 1)\n",
    "        \n",
    "        # Diferen√ßa (highlighting)\n",
    "        diff = np.abs(img_clahe_np - img_base_np)\n",
    "        diff = (diff - diff.min()) / (diff.max() - diff.min() + 1e-8)  # normalize\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(img_base_np)\n",
    "        axes[i, 0].set_title(f'Baseline (ID: {patient_id})', fontsize=11)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(img_clahe_np)\n",
    "        axes[i, 1].set_title('With CLAHE', fontsize=11)\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(diff, cmap='hot')\n",
    "        axes[i, 2].set_title('Difference Map', fontsize=11)\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Pipeline visualization saved to {save_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f369cd-2fa8-4292-afc7-0a6cbcb011c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_cropping_effect(dataset, num_samples=100):\n",
    "    \"\"\"Medir % de pixels removidos pelo crop\"\"\"\n",
    "    print(\"\\n Measuring Cropping Effect...\")\n",
    "    \n",
    "    pixels_removed = []\n",
    "    successful = 0\n",
    "    \n",
    "    for i in tqdm(range(num_samples)):\n",
    "        try:\n",
    "            # Obter ID do paciente\n",
    "            patient_id = str(dataset.data.iloc[i]['ID'])\n",
    "            \n",
    "            # Tentar left primeiro, depois right\n",
    "            img_path = None\n",
    "            for side in ['left', 'right']:\n",
    "                test_path = os.path.join(dataset.img_dir, f\"{patient_id}_{side}.jpg\")\n",
    "                if os.path.exists(test_path):\n",
    "                    img_path = test_path\n",
    "                    break\n",
    "            \n",
    "            if img_path is None:\n",
    "                continue  # Skip se n√£o encontrar nenhuma imagem\n",
    "            \n",
    "            # Carregar imagem\n",
    "            img_orig = cv2.imread(img_path)\n",
    "            \n",
    "            if img_orig is None:\n",
    "                continue  # Skip se falhar a ler\n",
    "            \n",
    "            orig_pixels = img_orig.shape[0] * img_orig.shape[1]\n",
    "            \n",
    "            # Aplicar crop (mesmo algoritmo que ApplyCLAHEandCrop)\n",
    "            gray = cv2.cvtColor(img_orig, cv2.COLOR_BGR2GRAY)\n",
    "            mask = gray > 10\n",
    "            \n",
    "            if np.any(mask):\n",
    "                coords = np.argwhere(mask)\n",
    "                y0, x0 = coords.min(axis=0)\n",
    "                y1, x1 = coords.max(axis=0) + 1\n",
    "                cropped_pixels = (y1 - y0) * (x1 - x0)\n",
    "            else:\n",
    "                cropped_pixels = orig_pixels\n",
    "            \n",
    "            removed = (orig_pixels - cropped_pixels) / orig_pixels * 100\n",
    "            pixels_removed.append(removed)\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue  # Skip imagens problem√°ticas\n",
    "    \n",
    "    if len(pixels_removed) > 0:\n",
    "        print(f\"  Successfully analyzed: {successful}/{num_samples} images\")\n",
    "        print(f\"  Mean pixels removed: {np.mean(pixels_removed):.1f}% ¬± {np.std(pixels_removed):.1f}%\")\n",
    "        print(f\"  Median: {np.median(pixels_removed):.1f}%\")\n",
    "        print(f\"  Range: {np.min(pixels_removed):.1f}% - {np.max(pixels_removed):.1f}%\")\n",
    "        print(f\"  Images with >20% removed: {(np.array(pixels_removed) > 20).sum()}/{successful}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Error: Could not analyze any images\")\n",
    "        print(f\"  Check if dataset.img_dir is correct: {dataset.img_dir}\")\n",
    "    \n",
    "    return pixels_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77caa2d7-4bdf-404a-a2d3-fbc8283c596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_clahe_effect(num_samples=50, save_path='results/clahe_effect.png'):\n",
    "    \"\"\"Medir aumento de contraste do CLAHE usando datasets\"\"\"\n",
    "    print(\"\\n Measuring CLAHE Effect...\")\n",
    "    \n",
    "    contrasts_before = []\n",
    "    contrasts_after = []\n",
    "    \n",
    "    # Usar os datasets que J√Å FUNCIONAM no teu c√≥digo\n",
    "    print(\"Loading datasets...\")\n",
    "    dataset_baseline = ODIRDataset('train', transform=get_val_test_transform())\n",
    "    dataset_clahe = ODIRDataset('train', transform=get_val_test_transform_CLAHE())\n",
    "    \n",
    "    print(f\"Analyzing {num_samples} images...\")\n",
    "    successful = 0\n",
    "    \n",
    "    for i in tqdm(range(min(num_samples, len(dataset_baseline)))):\n",
    "        try:\n",
    "            # Baseline (sem CLAHE)\n",
    "            img_base, _, _ = dataset_baseline[i]\n",
    "            # Converter tensor para numpy\n",
    "            img_base_np = img_base.permute(1, 2, 0).numpy()\n",
    "            # Denormalizar\n",
    "            img_base_np = img_base_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            img_base_np = np.clip(img_base_np * 255, 0, 255).astype(np.uint8)\n",
    "            gray_base = cv2.cvtColor(img_base_np, cv2.COLOR_RGB2GRAY)\n",
    "            contrasts_before.append(gray_base.std())\n",
    "            \n",
    "            # Com CLAHE\n",
    "            img_clahe, _, _ = dataset_clahe[i]\n",
    "            img_clahe_np = img_clahe.permute(1, 2, 0).numpy()\n",
    "            img_clahe_np = img_clahe_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            img_clahe_np = np.clip(img_clahe_np * 255, 0, 255).astype(np.uint8)\n",
    "            gray_clahe = cv2.cvtColor(img_clahe_np, cv2.COLOR_RGB2GRAY)\n",
    "            contrasts_after.append(gray_clahe.std())\n",
    "            \n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue  # Skip imagens problem√°ticas\n",
    "    \n",
    "    if len(contrasts_after) == 0:\n",
    "        print(\"‚ùå Error: Could not analyze any images\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Successfully analyzed: {successful}/{num_samples} images\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Histograma comparativo\n",
    "    axes[0].hist(contrasts_before, bins=20, alpha=0.6, label='Before CLAHE', color='steelblue', edgecolor='black')\n",
    "    axes[0].hist(contrasts_after, bins=20, alpha=0.6, label='After CLAHE', color='coral', edgecolor='black')\n",
    "    axes[0].axvline(np.mean(contrasts_before), color='blue', linestyle='--', linewidth=2,\n",
    "                    label=f'Mean Before: {np.mean(contrasts_before):.1f}')\n",
    "    axes[0].axvline(np.mean(contrasts_after), color='red', linestyle='--', linewidth=2,\n",
    "                    label=f'Mean After: {np.mean(contrasts_after):.1f}')\n",
    "    axes[0].set_xlabel('Contrast (Std Dev of Intensity)', fontsize=11)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0].set_title('Contrast Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Scatter plot (before vs after)\n",
    "    axes[1].scatter(contrasts_before, contrasts_after, alpha=0.6, color='mediumseagreen', s=50, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Linha de \"no change\"\n",
    "    min_val = min(min(contrasts_before), min(contrasts_after))\n",
    "    max_val = max(max(contrasts_before), max(contrasts_after))\n",
    "    axes[1].plot([min_val, max_val], [min_val, max_val], \n",
    "                 'k--', label='No change line', linewidth=2)\n",
    "    \n",
    "    axes[1].set_xlabel('Contrast Before CLAHE', fontsize=11)\n",
    "    axes[1].set_ylabel('Contrast After CLAHE', fontsize=11)\n",
    "    axes[1].set_title('Per-Image Contrast Change', fontsize=12, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # Calcular melhoria\n",
    "    improvement = (np.mean(contrasts_after) - np.mean(contrasts_before)) / np.mean(contrasts_before) * 100\n",
    "    \n",
    "    plt.suptitle(f'CLAHE Effect Analysis (n={successful}) - Average Improvement: +{improvement:.1f}%', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"  Mean contrast before: {np.mean(contrasts_before):.2f} ¬± {np.std(contrasts_before):.2f}\")\n",
    "    print(f\"  Mean contrast after:  {np.mean(contrasts_after):.2f} ¬± {np.std(contrasts_after):.2f}\")\n",
    "    print(f\"  Average improvement:  +{improvement:.1f}%\")\n",
    "    print(f\"  Min improvement:      +{((min(contrasts_after) - np.mean(contrasts_before)) / np.mean(contrasts_before) * 100):.1f}%\")\n",
    "    print(f\"  Max improvement:      +{((max(contrasts_after) - np.mean(contrasts_before)) / np.mean(contrasts_before) * 100):.1f}%\")\n",
    "    \n",
    "    # Guardar m√©tricas\n",
    "    metrics = {\n",
    "        'n_samples': successful,\n",
    "        'contrast_before_mean': float(np.mean(contrasts_before)),\n",
    "        'contrast_before_std': float(np.std(contrasts_before)),\n",
    "        'contrast_after_mean': float(np.mean(contrasts_after)),\n",
    "        'contrast_after_std': float(np.std(contrasts_after)),\n",
    "        'improvement_pct': float(improvement)\n",
    "    }\n",
    "    \n",
    "    with open('results/clahe_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n Metrics saved to results/clahe_metrics.json\")\n",
    "    print(f\" Figure saved to {save_path}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091de658-cafd-4563-908c-2b0d1372ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== AN√ÅLISES ADICIONAIS ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE EDA & PREPROCESSING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Dataset Summary Table\n",
    "dataset_summary = print_dataset_summary()\n",
    "\n",
    "# 2. Multi-label Co-occurrence\n",
    "train_data = pd.read_csv(os.path.join(DATASET_ROOT, 'train', 'train_metadata.csv'))\n",
    "plot_disease_cooccurrence(train_data)\n",
    "analyze_multilabel_distribution(train_data)\n",
    "\n",
    "# 3. Image Quality Analysis\n",
    "train_dataset_basic = ODIRDataset('train', transform=get_val_test_transform())\n",
    "analyze_image_quality(train_dataset_basic, num_samples=500)\n",
    "\n",
    "# 4. Cropping Effect\n",
    "measure_cropping_effect(train_dataset_basic, num_samples=100)\n",
    "\n",
    "# 5. CLAHE Effect\n",
    "measure_clahe_effect(num_samples=50)\n",
    "\n",
    "# 6. Complete Pipeline Visualization\n",
    "train_dataset_clahe = ODIRDataset('train', transform=get_train_transform_CLAHE())\n",
    "visualize_complete_pipeline(num_samples=3)\n",
    "\n",
    "print(\"\\n All EDA and Preprocessing analyses completed!\")\n",
    "print(\"   Check 'results/' folder for saved figures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e679ee-d1c1-4150-aaaf-9cb217c7f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'ODIRDataset' in dir(), \"‚ùå ODIRDataset n√£o encontrado!\"\n",
    "assert 'ApplyCLAHEandCrop_Adaptive' in dir(), \"‚ùå ApplyCLAHEandCrop_Adaptive n√£o encontrado!\"\n",
    "\n",
    "print(\"‚úÖ ODIRDataset: OK\")\n",
    "print(\"‚úÖ ApplyCLAHEandCrop_Adaptive: OK\")\n",
    "print(\"‚úÖ CropOnly: OK\" if 'CropOnly' in dir() else \"‚ö†Ô∏è CropOnly n√£o definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe953da4-e22d-49c0-9b12-96d1f8e9cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TRANSFORMA√á√ïES PARA TREINAR MODELOS ====================\n",
    "\n",
    "def get_v1_baseline_transform():\n",
    "    \"\"\"V1: Baseline puro (SEM crop, SEM aug, SEM CLAHE)\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_v2_crop_only_transform():\n",
    "    \"\"\"V2: S√≥ cropping (SEM aug, SEM CLAHE)\"\"\"\n",
    "    return transforms.Compose([\n",
    "        CropOnly(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_v3_crop_aug_transform():\n",
    "    \"\"\"V3: Cropping + Augmentation (SEM CLAHE)\"\"\"\n",
    "    return transforms.Compose([\n",
    "        CropOnly(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=5),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_v4_full_pipeline_transform():\n",
    "    \"\"\"V4: Full Pipeline (Crop + Aug + CLAHE)\"\"\"\n",
    "    return transforms.Compose([\n",
    "        ApplyCLAHEandCrop_Adaptive(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5d546-6418-4426-bb76-2caed717d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_for_config(config_name):\n",
    "    \"\"\"\n",
    "    Criar train/val/test datasets para uma configura√ß√£o\n",
    "    \n",
    "    Args:\n",
    "        config_name: 'v1', 'v2', 'v3', ou 'v4'\n",
    "    \n",
    "    Returns:\n",
    "        dict com keys 'train', 'val', 'test'\n",
    "    \"\"\"\n",
    "    \n",
    "    train_transforms = {\n",
    "        'v1': get_v1_baseline_transform(),\n",
    "        'v2': get_v2_crop_only_transform(),\n",
    "        'v3': get_v3_crop_aug_transform(),\n",
    "        'v4': get_v4_full_pipeline_transform()\n",
    "    }\n",
    "    \n",
    "    val_test_transforms = {\n",
    "        'v1': get_val_test_transform(),\n",
    "        'v2': get_val_test_transform(),\n",
    "        'v3': get_val_test_transform(),\n",
    "        'v4': get_val_test_transform_CLAHE()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüì¶ Criando datasets para config: {config_name.upper()}\")\n",
    "    \n",
    "    datasets = {\n",
    "        'train': ODIRDataset('train', transform=train_transforms[config_name]),\n",
    "        'val': ODIRDataset('val', transform=val_test_transforms[config_name]),\n",
    "        'test': ODIRDataset('test', transform=val_test_transforms[config_name])\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Datasets {config_name} criados!\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f1326-b34b-4adf-b5a0-242f1268319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VERIFICA√á√ÉO FINAL ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICA√á√ÉO - Tudo pronto para usar noutros notebooks\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checklist = {\n",
    "    'ODIRDataset': 'ODIRDataset' in dir(),\n",
    "    'ApplyCLAHEandCrop_Adaptive': 'ApplyCLAHEandCrop_Adaptive' in dir(),\n",
    "    'CropOnly': 'CropOnly' in dir(),\n",
    "    'get_v1_baseline_transform': 'get_v1_baseline_transform' in dir(),\n",
    "    'get_v2_crop_only_transform': 'get_v2_crop_only_transform' in dir(),\n",
    "    'get_v3_crop_aug_transform': 'get_v3_crop_aug_transform' in dir(),\n",
    "    'get_v4_full_pipeline_transform': 'get_v4_full_pipeline_transform' in dir(),\n",
    "    'get_val_test_transform': 'get_val_test_transform' in dir(),\n",
    "    'get_val_test_transform_CLAHE': 'get_val_test_transform_CLAHE' in dir(),\n",
    "    'create_datasets_for_config': 'create_datasets_for_config' in dir(),\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for item, exists in checklist.items():\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"{status} {item}\")\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nüéâ TUDO PRONTO! Podes usar %run neste notebook.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Alguns itens em falta. Verifica o c√≥digo acima.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654f4be-46de-4f64-b1aa-4f58e99d1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(dataset, thresholds=[30, 35, 40, 45, 50,  55, 60, 65, 70], n_samples=100):\n",
    "    \"\"\"\n",
    "    Testar diferentes thresholds para ver qual melhora mais\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Testing threshold={threshold}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        contrast_before = []\n",
    "        contrast_after = []\n",
    "        n_applied = 0  # Quantas imagens aplicaram CLAHE\n",
    "        \n",
    "        for i in range(min(n_samples, len(dataset))):\n",
    "            # Original\n",
    "            img, _, _ = dataset[i]\n",
    "            img_np = np.array(img)\n",
    "            \n",
    "            # Crop\n",
    "            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "            mask = gray > 10\n",
    "            if np.any(mask):\n",
    "                coords = np.argwhere(mask)\n",
    "                y0, x0 = coords.min(axis=0)\n",
    "                y1, x1 = coords.max(axis=0) + 1\n",
    "                img_cropped = img_np[y0:y1, x0:x1]\n",
    "            else:\n",
    "                img_cropped = img_np\n",
    "            \n",
    "            # Medir contraste ANTES\n",
    "            gray_before = cv2.cvtColor(img_cropped, cv2.COLOR_RGB2GRAY)\n",
    "            contrast_orig = gray_before.std()\n",
    "            contrast_before.append(contrast_orig)\n",
    "            \n",
    "            # Aplicar CLAHE adaptativo\n",
    "            if contrast_orig < threshold:\n",
    "                n_applied += 1\n",
    "                \n",
    "                lab = cv2.cvtColor(img_cropped, cv2.COLOR_RGB2LAB)\n",
    "                l, a, b = cv2.split(lab)\n",
    "                clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8,8))\n",
    "                l = clahe.apply(l)\n",
    "                img_final = cv2.merge([l, a, b])\n",
    "                img_final = cv2.cvtColor(img_final, cv2.COLOR_LAB2RGB)\n",
    "            else:\n",
    "                img_final = img_cropped\n",
    "            \n",
    "            # Medir contraste DEPOIS\n",
    "            gray_after = cv2.cvtColor(img_final, cv2.COLOR_RGB2GRAY)\n",
    "            contrast_after.append(gray_after.std())\n",
    "        \n",
    "        # Stats\n",
    "        before_mean = np.mean(contrast_before)\n",
    "        after_mean = np.mean(contrast_after)\n",
    "        improvement = (after_mean - before_mean) / before_mean * 100\n",
    "        pct_applied = n_applied / n_samples * 100\n",
    "        \n",
    "        print(f\"Contrast BEFORE: {before_mean:.2f}\")\n",
    "        print(f\"Contrast AFTER:  {after_mean:.2f}\")\n",
    "        print(f\"Improvement:     {improvement:+.2f}%\")\n",
    "        print(f\"CLAHE applied:   {n_applied}/{n_samples} ({pct_applied:.1f}%)\")\n",
    "        \n",
    "        results[threshold] = {\n",
    "            'improvement': improvement,\n",
    "            'pct_applied': pct_applied,\n",
    "            'before_mean': before_mean,\n",
    "            'after_mean': after_mean\n",
    "        }\n",
    "    \n",
    "    # Encontrar melhor\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    best_threshold = max(results.keys(), key=lambda k: results[k]['improvement'])\n",
    "    \n",
    "    for threshold, data in sorted(results.items()):\n",
    "        marker = \"‚≠ê BEST\" if threshold == best_threshold else \"\"\n",
    "        print(f\"Threshold {threshold}: {data['improvement']:+.2f}% \"\n",
    "              f\"({data['pct_applied']:.0f}% images) {marker}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ OPTIMAL: threshold={best_threshold}\")\n",
    "    print(f\"   Improvement: {results[best_threshold]['improvement']:+.2f}%\")\n",
    "    print(f\"   Applied to: {results[best_threshold]['pct_applied']:.0f}% of images\")\n",
    "    \n",
    "    return best_threshold, results\n",
    "\n",
    "#dataset = ODIRDataset('train', transform=None)\n",
    "#optimal_threshold, results = find_optimal_threshold(dataset, n_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbaf8fd-2b79-4cd3-8d4d-dc3e89830785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==================== VALIDAR clipLimit √ìTIMO ====================\n",
    "\n",
    "def find_optimal_cliplimit(dataset, threshold=50, clip_limits=[0.5, 1.0, 1.5, 2.0, 2.5], n_samples=500):\n",
    "    \"\"\"\n",
    "    Testar diferentes clipLimits com threshold fixo\n",
    "    \n",
    "    Args:\n",
    "        threshold: Threshold otimizado (fixo em 50)\n",
    "        clip_limits: Valores de clipLimit para testar\n",
    "        n_samples: N√∫mero de amostras\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TESTING clipLimit (threshold={threshold} FIXED)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for clip_limit in clip_limits:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Testing clipLimit={clip_limit}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        contrast_before = []\n",
    "        contrast_after = []\n",
    "        n_applied = 0\n",
    "        \n",
    "        for i in range(min(n_samples, len(dataset))):\n",
    "            # Original\n",
    "            img, _, _ = dataset[i]\n",
    "            img_np = np.array(img)\n",
    "            \n",
    "            # Crop\n",
    "            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "            mask = gray > 10\n",
    "            if np.any(mask):\n",
    "                coords = np.argwhere(mask)\n",
    "                y0, x0 = coords.min(axis=0)\n",
    "                y1, x1 = coords.max(axis=0) + 1\n",
    "                img_cropped = img_np[y0:y1, x0:x1]\n",
    "            else:\n",
    "                img_cropped = img_np\n",
    "            \n",
    "            # Medir contraste ANTES\n",
    "            gray_before = cv2.cvtColor(img_cropped, cv2.COLOR_RGB2GRAY)\n",
    "            contrast_orig = gray_before.std()\n",
    "            contrast_before.append(contrast_orig)\n",
    "            \n",
    "            # Aplicar CLAHE adaptativo com clipLimit vari√°vel\n",
    "            if contrast_orig < threshold:\n",
    "                n_applied += 1\n",
    "                \n",
    "                lab = cv2.cvtColor(img_cropped, cv2.COLOR_RGB2LAB)\n",
    "                l, a, b = cv2.split(lab)\n",
    "                \n",
    "                # CLAHE com clipLimit testado\n",
    "                clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8,8))\n",
    "                l = clahe.apply(l)\n",
    "                \n",
    "                img_final = cv2.merge([l, a, b])\n",
    "                img_final = cv2.cvtColor(img_final, cv2.COLOR_LAB2RGB)\n",
    "            else:\n",
    "                img_final = img_cropped\n",
    "            \n",
    "            # Medir contraste DEPOIS\n",
    "            gray_after = cv2.cvtColor(img_final, cv2.COLOR_RGB2GRAY)\n",
    "            contrast_after.append(gray_after.std())\n",
    "        \n",
    "        # Stats\n",
    "        before_mean = np.mean(contrast_before)\n",
    "        before_std = np.std(contrast_before)\n",
    "        after_mean = np.mean(contrast_after)\n",
    "        after_std = np.std(contrast_after)\n",
    "        \n",
    "        improvement = (after_mean - before_mean) / before_mean * 100\n",
    "        std_reduction = (before_std - after_std) / before_std * 100\n",
    "        pct_applied = n_applied / n_samples * 100\n",
    "        \n",
    "        print(f\"Contrast BEFORE: {before_mean:.2f} ¬± {before_std:.2f}\")\n",
    "        print(f\"Contrast AFTER:  {after_mean:.2f} ¬± {after_std:.2f}\")\n",
    "        print(f\"Improvement:     {improvement:+.2f}%\")\n",
    "        print(f\"STD reduction:   {std_reduction:.1f}%\")\n",
    "        print(f\"CLAHE applied:   {n_applied}/{n_samples} ({pct_applied:.1f}%)\")\n",
    "        \n",
    "        results[clip_limit] = {\n",
    "            'improvement': improvement,\n",
    "            'std_reduction': std_reduction,\n",
    "            'pct_applied': pct_applied,\n",
    "            'before_mean': before_mean,\n",
    "            'after_mean': after_mean,\n",
    "            'before_std': before_std,\n",
    "            'after_std': after_std\n",
    "        }\n",
    "    \n",
    "    # Encontrar melhor\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    best_clip = max(results.keys(), key=lambda k: results[k]['improvement'])\n",
    "    \n",
    "    print(f\"\\nclipLimit comparison (threshold={threshold}, applied to {results[list(results.keys())[0]]['pct_applied']:.0f}% images):\")\n",
    "    print(f\"{'clipLimit':<12} {'Improvement':<15} {'STD reduction':<15} {'Best?'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for clip_limit, data in sorted(results.items()):\n",
    "        marker = \"‚≠ê BEST\" if clip_limit == best_clip else \"\"\n",
    "        std_marker = \"‚ö†Ô∏è High\" if data['std_reduction'] > 20 else \"\"\n",
    "        \n",
    "        print(f\"{clip_limit:<12} {data['improvement']:+.2f}%{'':<11} \"\n",
    "              f\"{data['std_reduction']:.1f}%{'':<11} {marker} {std_marker}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ OPTIMAL: clipLimit={best_clip}\")\n",
    "    print(f\"   Improvement: {results[best_clip]['improvement']:+.2f}%\")\n",
    "    print(f\"   STD reduction: {results[best_clip]['std_reduction']:.1f}%\")\n",
    "    \n",
    "    # Warnings\n",
    "    if results[best_clip]['std_reduction'] > 25:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: STD reduction > 25%\")\n",
    "        print(f\"   clipLimit={best_clip} pode estar removendo variabilidade demais\")\n",
    "        print(f\"   Considerar clipLimit mais baixo\")\n",
    "    \n",
    "    return best_clip, results\n",
    "\n",
    "\n",
    "# ==================== EXECUTAR ====================\n",
    "\n",
    "#dataset = ODIRDataset('train', transform=None)\n",
    "\n",
    "# Testar clipLimits comuns\n",
    "#clip_limits = [0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
    "\n",
    "#optimal_clip, results = find_optimal_cliplimit(\n",
    "#    dataset,\n",
    "#    threshold=50,      # ‚Üê Threshold j√° otimizado\n",
    "#    clip_limits=clip_limits,\n",
    "#    n_samples=2500\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3fe3c7-79ab-4300-a0a6-d9fa1dbbaa20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
